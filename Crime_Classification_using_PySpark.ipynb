{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMhViaNbFGCWDlbFMfXFRfV"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rlkz-OMKzRfw",
        "outputId": "e884a2e1-57b8-4c4a-c29f-27385c546106"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317145 sha256=ed3c4b7dd1cbd6d1f8f185f3e4a4bb8c9b4ec9e7124ef48a94ffe23b3171289e\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/1b/4b/3363a1d04368e7ff0d408e57ff57966fcdf00583774e761327\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GOMS5feczIEG"
      },
      "outputs": [],
      "source": [
        "import pyspark\n",
        "spark = pyspark.sql.SparkSession.builder.appName(\"clipper-pyspark\").getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lha kaggle.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIoafpGh00m0",
        "outputId": "ca91453b-f872-45e3-aca9-e50dd6994856"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 74 May  5 12:45 kaggle.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q kaggle"
      ],
      "metadata": {
        "id": "bZZsFGJC1I50"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle"
      ],
      "metadata": {
        "id": "AU3f5Kho1MOk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp kaggle.json ~/.kaggle/"
      ],
      "metadata": {
        "id": "FXcjMj911QtM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "knOL-gzv1asE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWlYh0vr1d9b",
        "outputId": "2208aa4b-7371-4c65-dbaa-3bac02bda55a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d kaggle/san-francisco-crime-classification"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVc1Zxyw1g4j",
        "outputId": "bf302696-528a-4801-cb2f-1f0a7be3b308"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading san-francisco-crime-classification.zip to /content\n",
            " 92% 38.0M/41.3M [00:00<00:00, 86.5MB/s]\n",
            "100% 41.3M/41.3M [00:00<00:00, 88.3MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/san-francisco-crime-classification.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iiQincOe1sKr",
        "outputId": "4b14154d-6976-40ae-b1b3-2553e6cd2984"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/san-francisco-crime-classification.zip\n",
            "  inflating: test.csv                \n",
            "  inflating: train.csv               \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "np.random.seed(60)"
      ],
      "metadata": {
        "id": "PvyJFU62zcIj"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%sh\n",
        "#Let see the first 5 rows\n",
        "head -5 train.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5WEh8TyzzTe",
        "outputId": "c1269d73-b6e9-429e-e3af-1273c978bc49"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dates,Category,Descript,DayOfWeek,PdDistrict,Resolution,Address,X,Y\n",
            "2015-05-13 23:53:00,WARRANTS,WARRANT ARREST,Wednesday,NORTHERN,\"ARREST, BOOKED\",OAK ST / LAGUNA ST,-122.425891675136,37.7745985956747\n",
            "2015-05-13 23:53:00,OTHER OFFENSES,TRAFFIC VIOLATION ARREST,Wednesday,NORTHERN,\"ARREST, BOOKED\",OAK ST / LAGUNA ST,-122.425891675136,37.7745985956747\n",
            "2015-05-13 23:33:00,OTHER OFFENSES,TRAFFIC VIOLATION ARREST,Wednesday,NORTHERN,\"ARREST, BOOKED\",VANNESS AV / GREENWICH ST,-122.42436302145,37.8004143219856\n",
            "2015-05-13 23:30:00,LARCENY/THEFT,GRAND THEFT FROM LOCKED AUTO,Wednesday,NORTHERN,NONE,1500 Block of LOMBARD ST,-122.42699532676599,37.80087263276921\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Read the data into spark datafrome\n",
        "from pyspark.sql.functions import col, lower\n",
        "df = spark.read.format('csv')\\\n",
        "          .option('header','true')\\\n",
        "          .option('inferSchema', 'true')\\\n",
        "          .option('timestamp', 'true')\\\n",
        "          .load('train.csv')\n",
        "\n",
        "data = df.select(lower(col('Category')),lower(col('Descript')))\\\n",
        "        .withColumnRenamed('lower(Category)','Category')\\\n",
        "        .withColumnRenamed('lower(Descript)', 'Description')\n",
        "data.cache()\n",
        "print('Dataframe Structure')\n",
        "print('----------------------------------')\n",
        "print(data.printSchema())\n",
        "print(' ')\n",
        "print('Dataframe preview')\n",
        "print(data.show(5))\n",
        "print(' ')\n",
        "print('----------------------------------')\n",
        "print('Total number of rows', df.count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGJBkyZfz2go",
        "outputId": "06e1d96a-eda6-4686-d3f1-bb8250bc3d63"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataframe Structure\n",
            "----------------------------------\n",
            "root\n",
            " |-- Category: string (nullable = true)\n",
            " |-- Description: string (nullable = true)\n",
            "\n",
            "None\n",
            " \n",
            "Dataframe preview\n",
            "+--------------+--------------------+\n",
            "|      Category|         Description|\n",
            "+--------------+--------------------+\n",
            "|      warrants|      warrant arrest|\n",
            "|other offenses|traffic violation...|\n",
            "|other offenses|traffic violation...|\n",
            "| larceny/theft|grand theft from ...|\n",
            "| larceny/theft|grand theft from ...|\n",
            "+--------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "None\n",
            " \n",
            "----------------------------------\n",
            "Total number of rows 878049\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def top_n_list(df,var, N):\n",
        "    '''\n",
        "    This function determine the top N numbers of the list\n",
        "    '''\n",
        "    print(\"Total number of unique value of\"+' '+var+''+':'+' '+str(df.select(var).distinct().count()))\n",
        "    print(' ')\n",
        "    print('Top'+' '+str(N)+' '+'Crime'+' '+var)\n",
        "    df.groupBy(var).count().withColumnRenamed('count','totalValue')\\\n",
        "    .orderBy(col('totalValue').desc()).show(N)\n",
        "    \n",
        "    \n",
        "top_n_list(data, 'Category',10)\n",
        "print(' ')\n",
        "print(' ')\n",
        "top_n_list(data,'Description',10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ifb2BppC12fe",
        "outputId": "d1e2def2-48f3-47f8-9e1c-eb403de0b97d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of unique value of Category: 39\n",
            " \n",
            "Top 10 Crime Category\n",
            "+--------------+----------+\n",
            "|      Category|totalValue|\n",
            "+--------------+----------+\n",
            "| larceny/theft|    174900|\n",
            "|other offenses|    126182|\n",
            "|  non-criminal|     92304|\n",
            "|       assault|     76876|\n",
            "| drug/narcotic|     53971|\n",
            "| vehicle theft|     53781|\n",
            "|     vandalism|     44725|\n",
            "|      warrants|     42214|\n",
            "|      burglary|     36755|\n",
            "|suspicious occ|     31414|\n",
            "+--------------+----------+\n",
            "only showing top 10 rows\n",
            "\n",
            " \n",
            " \n",
            "Total number of unique value of Description: 879\n",
            " \n",
            "Top 10 Crime Description\n",
            "+--------------------+----------+\n",
            "|         Description|totalValue|\n",
            "+--------------------+----------+\n",
            "|grand theft from ...|     60022|\n",
            "|       lost property|     31729|\n",
            "|             battery|     27441|\n",
            "|   stolen automobile|     26897|\n",
            "|drivers license, ...|     26839|\n",
            "|      warrant arrest|     23754|\n",
            "|suspicious occurr...|     21891|\n",
            "|aided case, menta...|     21497|\n",
            "|petty theft from ...|     19771|\n",
            "|malicious mischie...|     17789|\n",
            "+--------------------+----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.select('Category').distinct().count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7hNAOxj16Qy",
        "outputId": "6411e87b-1e83-4bbd-aba6-60b1df885c10"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "39"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training, test = data.randomSplit([0.7,0.3], seed=60)\n",
        "#trainingSet.cache()\n",
        "print(\"Training Dataset Count:\", training.count())\n",
        "print(\"Test Dataset Count:\", test.count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUwRd72s182r",
        "outputId": "1680926d-5253-4a9e-a6db-02eac262341e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Dataset Count: 614667\n",
            "Test Dataset Count: 263382\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Define Structure to build Pipeline**\n",
        "The process of cleaning the dataset involves:\n",
        "\n",
        "Define tokenization function using RegexTokenizer: RegexTokenizer allows more advanced tokenization based on regular expression (regex) matching. By default, the parameter “pattern” (regex, default: “\\s+”) is used as delimiters to split the input text. Alternatively, users can set parameter “gaps” to false indicating the regex “pattern” denotes “tokens” rather than splitting gaps, and find all matching occurrences as the tokenization result.\n",
        "\n",
        "Define stop remover function using StopWordsRemover: StopWordsRemover takes as input a sequence of strings (e.g. the output of a Tokenizer) and drops all the stop words from the input sequences. The list of stopwords is specified by the stopWords parameter.\n",
        "\n",
        "Define bag of words function for Descript variable using CountVectorizer: CountVectorizer can be used as an estimator to extract the vocabulary, and generates a CountVectorizerModel. The model produces sparse representations for the documents over the vocabulary, which can then be passed to other algorithms like LDA. During the fitting process, CountVectorizer will select the top vocabSize words ordered by term frequency across the corpus. An optional parameter minDF also affects the fitting process by specifying the minimum number (or fraction if < 1.0) of documents a term must appear in to be included in the vocabulary.\n",
        "\n",
        "Define function to Encode the values of category variable using StringIndexer: StringIndexer encodes a string column of labels to a column of label indices. The indices are in (0, numLabels), ordered by label frequencies, so the most frequent label gets index 0. In our case, the label colum(Category) will be encoded to label indices, from 0 to 38; the most frequent label (LARCENY/THEFT) will be indexed as 0.\n",
        "\n",
        "Define a pipeline to call these functions: ML Pipelines provide a uniform set of high-level APIs built on top of DataFrames that help users create and tune practical machine learning pipelines."
      ],
      "metadata": {
        "id": "bddY4S3X2Ooj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer, OneHotEncoder, StringIndexer, VectorAssembler, HashingTF, IDF, Word2Vec\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import LogisticRegression, NaiveBayes \n",
        "\n",
        "#----------------Define tokenizer with regextokenizer()------------------\n",
        "regex_tokenizer = RegexTokenizer(pattern='\\\\W')\\\n",
        "                  .setInputCol(\"Description\")\\\n",
        "                  .setOutputCol(\"tokens\")\n",
        "\n",
        "#----------------Define stopwords with stopwordsremover()---------------------\n",
        "extra_stopwords = ['http','amp','rt','t','c','the']\n",
        "stopwords_remover = StopWordsRemover()\\\n",
        "                    .setInputCol('tokens')\\\n",
        "                    .setOutputCol('filtered_words')\\\n",
        "                    .setStopWords(extra_stopwords)\n",
        "                    \n",
        "\n",
        "#----------Define bags of words using countVectorizer()---------------------------\n",
        "count_vectors = CountVectorizer(vocabSize=10000, minDF=5)\\\n",
        "               .setInputCol(\"filtered_words\")\\\n",
        "               .setOutputCol(\"features\")\n",
        "\n",
        "\n",
        "#-----------Using TF-IDF to vectorise features instead of countVectoriser-----------------\n",
        "hashingTf = HashingTF(numFeatures=10000)\\\n",
        "            .setInputCol(\"filtered_words\")\\\n",
        "            .setOutputCol(\"raw_features\")\n",
        "            \n",
        "#Use minDocFreq to remove sparse terms\n",
        "idf = IDF(minDocFreq=5)\\\n",
        "        .setInputCol(\"raw_features\")\\\n",
        "        .setOutputCol(\"features\")\n",
        "\n",
        "#---------------Define bag of words using Word2Vec---------------------------\n",
        "word2Vec = Word2Vec(vectorSize=1000, minCount=0)\\\n",
        "           .setInputCol(\"filtered_words\")\\\n",
        "           .setOutputCol(\"features\")\n",
        "\n",
        "#-----------Encode the Category variable into label using StringIndexer-----------\n",
        "label_string_idx = StringIndexer()\\\n",
        "                  .setInputCol(\"Category\")\\\n",
        "                  .setOutputCol(\"label\")\n",
        "\n",
        "#-----------Define classifier structure for logistic Regression--------------\n",
        "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
        "\n",
        "#---------Define classifier structure for Naive Bayes----------\n",
        "nb = NaiveBayes(smoothing=1)\n",
        "\n",
        "def metrics_ev(labels, metrics):\n",
        "    '''\n",
        "    List of all performance metrics\n",
        "    '''\n",
        "    # Confusion matrix\n",
        "    print(\"---------Confusion matrix-----------------\")\n",
        "    print(metrics.confusionMatrix)\n",
        "    print(' ')    \n",
        "    # Overall statistics\n",
        "    print('----------Overall statistics-----------')\n",
        "    print(\"Precision = %s\" %  metrics.precision())\n",
        "    print(\"Recall = %s\" %  metrics.recall())\n",
        "    print(\"F1 Score = %s\" % metrics.fMeasure())\n",
        "    print(' ')\n",
        "    # Statistics by class\n",
        "    print('----------Statistics by class----------')\n",
        "    for label in sorted(labels):\n",
        "       print(\"Class %s precision = %s\" % (label, metrics.precision(label)))\n",
        "       print(\"Class %s recall = %s\" % (label, metrics.recall(label)))\n",
        "       print(\"Class %s F1 Measure = %s\" % (label, metrics.fMeasure(label, beta=1.0)))\n",
        "    print(' ')\n",
        "    # Weighted stats\n",
        "    print('----------Weighted stats----------------')\n",
        "    print(\"Weighted recall = %s\" % metrics.weightedRecall)\n",
        "    print(\"Weighted precision = %s\" % metrics.weightedPrecision)\n",
        "    print(\"Weighted F(1) Score = %s\" % metrics.weightedFMeasure())\n",
        "    print(\"Weighted F(0.5) Score = %s\" % metrics.weightedFMeasure(beta=0.5))\n",
        "    print(\"Weighted false positive rate = %s\" % metrics.weightedFalsePositiveRate)\n",
        "    "
      ],
      "metadata": {
        "id": "wP58STGO2N30"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Build Multi-Classification**\n",
        "\n",
        "The stages involve to perform multi-classification include:\n",
        "\n",
        "Model training and evaluation\n",
        "Build baseling model\n",
        "Logistic regression using CountVectorizer features\n",
        "Build secondary models\n",
        "Naive Bayes\n",
        "Logistic regression and Naive Bayes using TF-IDF features\n",
        "Logistic regression and Naive Bayes using word2Vec"
      ],
      "metadata": {
        "id": "gW_ul7D12f78"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(i) Baseline Model**\n",
        "\n",
        "Baseline model should be quick, low cost and simple to setup and produce a decent results. One of the reason to consider baselines because they iterate very quickly, while wasting minimal time. To further undertand why and how to apply baselines, please refer to Emmanuel Ameisen's article: Always start with a stupid model, no exceptions.\n",
        "\n",
        "(a). Apply Logistic Regression with Count Vector Features\n",
        "We will build a model to make predictions and score on the test sets using logistics regression using the dataset we transformed using count vectors. And we will see the top 10 predictions from the highest probability from our model, accuracy and other metrics to evaluate our model.\n",
        "\n",
        "Note: Fit regex_tokenizer,stopwords_remover,count_vectors,label_string_idx, and lr functions into pipeline."
      ],
      "metadata": {
        "id": "RsBGFwsY2mZU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_cv_lr = Pipeline().setStages([regex_tokenizer,stopwords_remover,count_vectors,label_string_idx, lr])\n",
        "model_cv_lr = pipeline_cv_lr.fit(training)\n",
        "predictions_cv_lr = model_cv_lr.transform(test)"
      ],
      "metadata": {
        "id": "OmuVzwIx2ADM"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('-----------------------------Check Top 5 predictions----------------------------------')\n",
        "print(' ')\n",
        "predictions_cv_lr.select('Description','Category',\"probability\",\"label\",\"prediction\")\\\n",
        "                                        .orderBy(\"probability\", ascending=False)\\\n",
        "                                        .show(n=5, truncate=30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QcToqWqB2rBs",
        "outputId": "e4f46fa6-5e1b-4397-e8c9-65dd755e15cb"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------Check Top 5 predictions----------------------------------\n",
            " \n",
            "+------------------------------+-------------+------------------------------+-----+----------+\n",
            "|                   Description|     Category|                   probability|label|prediction|\n",
            "+------------------------------+-------------+------------------------------+-----+----------+\n",
            "|theft, bicycle, <$50, no se...|larceny/theft|[0.8738390105609033,0.02048...|  0.0|       0.0|\n",
            "|theft, bicycle, <$50, no se...|larceny/theft|[0.8738390105609033,0.02048...|  0.0|       0.0|\n",
            "|theft, bicycle, <$50, no se...|larceny/theft|[0.8738390105609033,0.02048...|  0.0|       0.0|\n",
            "|theft, bicycle, <$50, no se...|larceny/theft|[0.8738390105609033,0.02048...|  0.0|       0.0|\n",
            "|theft, bicycle, <$50, no se...|larceny/theft|[0.8738390105609033,0.02048...|  0.0|       0.0|\n",
            "+------------------------------+-------------+------------------------------+-----+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator \n",
        "evaluator_cv_lr = MulticlassClassificationEvaluator().setPredictionCol(\"prediction\").evaluate(predictions_cv_lr)\n",
        "print(' ')\n",
        "print('------------------------------Accuracy----------------------------------')\n",
        "print(' ')\n",
        "print('                       accuracy:{}:'.format(evaluator_cv_lr))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JB8nanb2uei",
        "outputId": "2a3fd50f-30d4-49ae-9a5a-e98acec73fb4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " \n",
            "------------------------------Accuracy----------------------------------\n",
            " \n",
            "                       accuracy:0.9720379224200315:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(ii). Secondary Models\n",
        "\n",
        "(a). Apply Naive Bayes with Count Vector Features\n",
        "Naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes’ theorem with strong (naive) independence assumptions between the features. The spark.ml implementation currently supports both multinomial naive Bayes and Bernoulli naive Bayes.\n",
        "\n",
        "Fit regex_tokenizer,stopwords_remover,count_vectors,label_string_idx, and nb functions into pipeline."
      ],
      "metadata": {
        "id": "5E1zq52z2zdz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Secondary model using NaiveBayes\n",
        "pipeline_cv_nb = Pipeline().setStages([regex_tokenizer,stopwords_remover,count_vectors,label_string_idx, nb])\n",
        "model_cv_nb = pipeline_cv_nb.fit(training)\n",
        "predictions_cv_nb = model_cv_nb.transform(test)"
      ],
      "metadata": {
        "id": "1WwONHlo2xFm"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator_cv_nb = MulticlassClassificationEvaluator().setPredictionCol(\"prediction\").evaluate(predictions_cv_nb)\n",
        "print(' ')\n",
        "print('--------------------------Accuracy-----------------------------')\n",
        "print(' ')\n",
        "print('                      accuracy:{}:'.format(evaluator_cv_nb))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mi-Ehe6s24Ec",
        "outputId": "20fb47f3-e581-4106-8306-fb440fae0998"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " \n",
            "--------------------------Accuracy-----------------------------\n",
            " \n",
            "                      accuracy:0.99350875457078:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(b). Apply Logistic Regression Using TF-IDF Features\n",
        "Term frequency-inverse document frequency (TF-IDF) is a feature vectorization method widely used in text mining to reflect the importance of a term to a document in the corpus. Denote a term by t, a document by d, and the corpus by D. Term frequency TF(t,d) is the number of times that term t appears in document d, while document frequency DF(t,D) is the number of documents that contains term t. If we only use term frequency to measure the importance, it is very easy to over-emphasize terms that appear very often but carry little information about the document, e.g. “a”, “the”, and “of”. If a term appears very often across the corpus, it means it doesn’t carry special information about a particular document. Inverse document frequency is a numerical measure of how much information a term provides:\n",
        " \n",
        ", where |D| is the total number of documents in the corpus. Since logarithm is used, if a term appears in all documents, its IDF value becomes 0. Note that a smoothing term is applied to avoid dividing by zero for terms outside the corpus. The TF-IDF measure is simply the product of TF and IDF:\n",
        ".\n",
        "\n",
        "There are several variants on the definition of term frequency and document frequency. In MLlib, we separate TF and IDF to make them flexible.\n",
        "\n",
        "Note: Fit regex_tokenizer, stopwords_remover,hashingTF, idf,label_string_idx, and nb functions into pipeline."
      ],
      "metadata": {
        "id": "jmb9vK_n29kT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_idf_lr = Pipeline().setStages([regex_tokenizer,stopwords_remover,hashingTf, idf, label_string_idx, lr])\n",
        "model_idf_lr = pipeline_idf_lr.fit(training)\n",
        "predictions_idf_lr = model_idf_lr.transform(test)"
      ],
      "metadata": {
        "id": "sOmfsiUN259E"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('-----------------------------Check Top 5 predictions----------------------------------')\n",
        "print(' ')\n",
        "predictions_idf_lr.select('Description','Category',\"probability\",\"label\",\"prediction\")\\\n",
        "                                        .orderBy(\"probability\", ascending=False)\\\n",
        "                                        .show(n=5, truncate=30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TQqlWfz3A7-",
        "outputId": "afaf5396-37fd-4868-bc13-5189783d82c2"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------Check Top 5 predictions----------------------------------\n",
            " \n",
            "+------------------------------+-------------+------------------------------+-----+----------+\n",
            "|                   Description|     Category|                   probability|label|prediction|\n",
            "+------------------------------+-------------+------------------------------+-----+----------+\n",
            "|theft, bicycle, <$50, no se...|larceny/theft|[0.8845322339589867,0.01879...|  0.0|       0.0|\n",
            "|theft, bicycle, <$50, no se...|larceny/theft|[0.8845322339589867,0.01879...|  0.0|       0.0|\n",
            "|theft, bicycle, <$50, no se...|larceny/theft|[0.8845322339589867,0.01879...|  0.0|       0.0|\n",
            "|theft, bicycle, <$50, no se...|larceny/theft|[0.8845322339589867,0.01879...|  0.0|       0.0|\n",
            "|theft, bicycle, <$50, no se...|larceny/theft|[0.8845322339589867,0.01879...|  0.0|       0.0|\n",
            "+------------------------------+-------------+------------------------------+-----+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator_idf_lr = MulticlassClassificationEvaluator().setPredictionCol(\"prediction\").evaluate(predictions_idf_lr)\n",
        "print(' ')\n",
        "print('-------------------------------Accuracy---------------------------------')\n",
        "print(' ')\n",
        "print('                        accuracy:{}:'.format(evaluator_idf_lr))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORVrehTL3D_a",
        "outputId": "351c4624-5cd3-45a3-d36d-e375aa9905f9"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " \n",
            "-------------------------------Accuracy---------------------------------\n",
            " \n",
            "                        accuracy:0.9719229068954107:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Apply Naive Bayes with TF-IDF Features"
      ],
      "metadata": {
        "id": "aZn7af4W3I8U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_idf_nb = Pipeline().setStages([regex_tokenizer,stopwords_remover,hashingTf, idf, label_string_idx, nb])\n",
        "model_idf_nb = pipeline_idf_nb.fit(training)\n",
        "predictions_idf_nb = model_idf_nb.transform(test)"
      ],
      "metadata": {
        "id": "aD6iGxrX3G47"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator_idf_nb = MulticlassClassificationEvaluator().setPredictionCol(\"prediction\").evaluate(predictions_idf_nb)\n",
        "print(' ')\n",
        "print('-----------------------------Accuracy-----------------------------')\n",
        "print(' ')\n",
        "print('                          accuracy:{}:'.format(evaluator_idf_nb))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8EfZQLkd3LOV",
        "outputId": "3050a095-3031-4c80-d83e-f1fa81c51ff9"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " \n",
            "-----------------------------Accuracy-----------------------------\n",
            " \n",
            "                          accuracy:0.994973005560035:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!-- (e). Apply Logistic Regression Using Word2Vec features\n",
        "Word2Vec is an Estimator which takes sequences of words representing documents and trains a Word2VecModel. The model maps each word to a unique fixed-size vector. The Word2VecModel transforms each document into a vector using the average of all words in the document; this vector can then be used as features for prediction, document similarity calculations, etc. -->"
      ],
      "metadata": {
        "id": "oanSplfb3QeD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pipeline_wv_nb = Pipeline().setStages([regex_tokenizer,stopwords_remover, word2Vec, label_string_idx, nb])\n",
        "# model_wv_nb = pipeline_wv_nb.fit(training)\n",
        "# predictions_wv_nb = model_wv_nb.transform(test)"
      ],
      "metadata": {
        "id": "4_ZzbX247kN7"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluator_wv_nb = MulticlassClassificationEvaluator().setPredictionCol(\"prediction\").evaluate(predictions_wv_nb)\n",
        "# print('--------Accuracy------------')\n",
        "# print(' ')\n",
        "# print('accuracy:{}%:'.format(round(evaluator_wv_nb *100),2))"
      ],
      "metadata": {
        "id": "MuYzTju97nmd"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJlg4tzW_l0y",
        "outputId": "a3c8b1c5-c65d-4b33-dfe3-2b2b11890d47"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (2.2.4)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask) (8.1.3)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from flask) (2.3.0)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from flask) (3.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->flask) (2.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, jsonify\n",
        "from pyspark.ml import PipelineModel"
      ],
      "metadata": {
        "id": "PfG5KgGP3VK4"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "app = Flask(__name__)"
      ],
      "metadata": {
        "id": "8bAOtUZq69Iq"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Model\n",
        "MODEL=pyspark.ml.PipelineModel(\"spark-naive-bayes-model\")"
      ],
      "metadata": {
        "id": "oNetHA346_QS"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HTTP_BAD_REQUEST = 400"
      ],
      "metadata": {
        "id": "5-kUXOT-6_7x"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@app.route('/predict')\n",
        "def predict():\n",
        "    Description = request.args.get('Description', default=None, type=str)\n",
        "    \n",
        "    # Reject request that have bad or missing values.\n",
        "    if Description is None:\n",
        "        # Provide the caller with feedback on why the record is unscorable.\n",
        "        message = ('Record cannot be scored because of '\n",
        "                   'missing or unacceptable values. '\n",
        "                   'All values must be present and of type string.')\n",
        "        response = jsonify(status='error',\n",
        "                           error_message=message)\n",
        "        # Sets the status code to 400\n",
        "        response.status_code = HTTP_BAD_REQUEST\n",
        "        return response\n",
        "    \n",
        "    features = [[Description]]\n",
        "    predictions = MODEL.transform(features)\n",
        "    label_pred = predictions.select(\"Description\",\"Category\",\"probability\",\"prediction\")\n",
        "    return jsonify(status='complete', label=label_pred)"
      ],
      "metadata": {
        "id": "5YRWJ6gY7C-T"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    app.run(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQFWHMv77EGE",
        "outputId": "a86bb4e6-0d85-4a4f-cfcf-d055cd63fdf6"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: on\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug: * Restarting with stat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask\n",
        "from flask_ngrok import run_with_ngrok\n",
        "app = Flask(__name__)\n",
        "run_with_ngrok(app)   \n",
        "  \n",
        "@app.route(\"/\")\n",
        "def home():\n",
        "    return \"<h1>GFG is great platform to learn</h1>\"\n",
        "    \n",
        "app.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xilQjbdgA6KR",
        "outputId": "f70daac5-c201-4aa7-8a67-607dada23f0b"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "Exception in thread Thread-12:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1378, in run\n",
            "    self.function(*self.args, **self.kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flask_ngrok.py\", line 70, in start_ngrok\n",
            "    ngrok_address = _run_ngrok()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flask_ngrok.py\", line 36, in _run_ngrok\n",
            "    j = json.loads(tunnel_url)\n",
            "  File \"/usr/lib/python3.10/json/__init__.py\", line 346, in loads\n",
            "    return _default_decoder.decode(s)\n",
            "  File \"/usr/lib/python3.10/json/decoder.py\", line 337, in decode\n",
            "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
            "  File \"/usr/lib/python3.10/json/decoder.py\", line 355, in raw_decode\n",
            "    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n",
            "json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flask_ngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6RgmBVcA_Qo",
        "outputId": "c3b412bc-227a-478f-acbc-e4e509323d41"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting flask_ngrok\n",
            "  Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.10/dist-packages (from flask_ngrok) (2.2.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from flask_ngrok) (2.27.1)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask_ngrok) (2.3.0)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask_ngrok) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask_ngrok) (8.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask_ngrok) (3.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->flask_ngrok) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->flask_ngrok) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->flask_ngrok) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->flask_ngrok) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->Flask>=0.8->flask_ngrok) (2.1.2)\n",
            "Installing collected packages: flask_ngrok\n",
            "Successfully installed flask_ngrok-0.0.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flask --quiet\n",
        "!pip install flask-ngrok --quiet\n",
        "print(\"Completed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gv0b_9jM7F7b",
        "outputId": "2ee32ec6-4d2d-4bdc-b37e-a47b3be63071"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install ngrok linux version using the following command or you can get the\n",
        "# latest version from its official website- https://dashboard.ngrok.com/get-started/setup\n",
        "\n",
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.tgz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iK2SuN-vASMU",
        "outputId": "a207aa9c-6e42-415d-ae81-960eee4b574b"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-05 13:07:40--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.tgz\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 18.205.222.128, 54.237.133.81, 54.161.241.46, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|18.205.222.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13856790 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.tgz’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.21M  48.3MB/s    in 0.3s    \n",
            "\n",
            "2023-05-05 13:07:40 (48.3 MB/s) - ‘ngrok-stable-linux-amd64.tgz’ saved [13856790/13856790]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extract the downloaded file using the following command \n",
        "\n",
        "!tar -xvf /content/ngrok-stable-linux-amd64.tgz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OROinGw3Bl26",
        "outputId": "6af699f0-35d9-4e62-f996-1c7f1bd042a4"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ngrok\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# paste your AuthToken here and execute this command\n",
        "\n",
        "!./ngrok authtoken 23H0IY10fqeKMIW7kG05JhKZMae_3Zabr2iqkU9AUcZ7CrRTP"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKZx9mcwBony",
        "outputId": "d7200dbf-d064-4373-bfc6-58b3a65f9825"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import Flask from flask module\n",
        "from flask import Flask\n",
        "\n",
        "# import run_with_ngrok from flask_ngrok to run the app using ngrok\n",
        "from flask_ngrok import run_with_ngrok\n",
        "  \n",
        "app = Flask(__name__) #app name\n",
        "run_with_ngrok(app)\n",
        "  \n",
        "@app.route('/predict')\n",
        "def predict():\n",
        "    Description = request.args.get('Description', default=None, type=str)\n",
        "    \n",
        "    # Reject request that have bad or missing values.\n",
        "    if Description is None:\n",
        "        # Provide the caller with feedback on why the record is unscorable.\n",
        "        message = ('Record cannot be scored because of '\n",
        "                   'missing or unacceptable values. '\n",
        "                   'All values must be present and of type string.')\n",
        "        response = jsonify(status='error',\n",
        "                           error_message=message)\n",
        "        # Sets the status code to 400\n",
        "        response.status_code = HTTP_BAD_REQUEST\n",
        "        return response\n",
        "    \n",
        "    features = [[Description]]\n",
        "    predictions = MODEL.transform(features)\n",
        "    label_pred = predictions.select(\"Description\",\"Category\",\"probability\",\"prediction\")\n",
        "    return jsonify(status='complete', label=label_pred)\n",
        "  \n",
        "if __name__ == \"__main__\":\n",
        "    app.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PI4JKlh3Bqpb",
        "outputId": "2fdb039a-41c4-4221-facb-1afe31698c77"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "Exception in thread Thread-15:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1378, in run\n",
            "    self.function(*self.args, **self.kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flask_ngrok.py\", line 70, in start_ngrok\n",
            "    ngrok_address = _run_ngrok()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flask_ngrok.py\", line 36, in _run_ngrok\n",
            "    j = json.loads(tunnel_url)\n",
            "  File \"/usr/lib/python3.10/json/__init__.py\", line 346, in loads\n",
            "    return _default_decoder.decode(s)\n",
            "  File \"/usr/lib/python3.10/json/decoder.py\", line 337, in decode\n",
            "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
            "  File \"/usr/lib/python3.10/json/decoder.py\", line 355, in raw_decode\n",
            "    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n",
            "json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KGH7Uh5DBtL6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}